{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import dblp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all available volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Web = req.get('http://ceurspt.wikidata.dbis.rwth-aachen.de/index.html') \n",
    "  \n",
    "S = BeautifulSoup(Web.text, 'lxml') \n",
    "html_txt = S.prettify()\n",
    "#extract all volumes\n",
    "reg1 = r'Vol-(\\d+)\">'\n",
    "#all volumes from the ceurspt api\n",
    "volumes = re.findall(reg1, html_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all pages for each vol\n",
    "papers = {}\n",
    "for v in volumes:\n",
    "    url = 'http://ceurspt.wikidata.dbis.rwth-aachen.de/Vol-' + v \n",
    "    Web = req.get(url) \n",
    "    reg2 = r'paper(\\d+).pdf'\n",
    "    papers[int(v)] = re.findall(reg2, BeautifulSoup(Web.text, 'lxml').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Author_G:\n",
    "    firstname: str\n",
    "    middlename: str\n",
    "    surname: str\n",
    "    affiliation:str\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers[2462] = ['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrobitFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.grobidxml = BeautifulSoup(req.get(filename).text, 'lxml')\n",
    "        self._title = ''\n",
    "\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = self.grobidxml.title.getText()\n",
    "        return self._title\n",
    "\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.grobidxml.analytic.find_all('author')\n",
    "        result = []\n",
    "        authors_list = []\n",
    "        affiliations = []\n",
    "        emails = []\n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            affiliation = author.affiliation\n",
    "            if persname: \n",
    "                firstname = elem_to_text(persname.find(\"forename\", type=\"first\"))\n",
    "                middlename = elem_to_text(persname.find(\"forename\", type=\"middle\"))\n",
    "                surname = elem_to_text(persname.surname)\n",
    "                authors_list.append((firstname, middlename, surname))\n",
    "                emails.append(elem_to_text(author.email))\n",
    "                if affiliation:\n",
    "                    aff = ''\n",
    "                    aff += (elem_to_text(affiliation.find(\"orgname\", type = \"department\"))) + \" \" \n",
    "                    aff += (elem_to_text(affiliation.find(\"orgname\", type = \"institution\")))\n",
    "                    affiliations.append(aff)\n",
    "            elif affiliation: \n",
    "                aff = ''\n",
    "                aff += (elem_to_text(affiliation.find(\"orgname\", type = \"department\"))) + '\\n'\n",
    "                aff += (elem_to_text(affiliation.find(\"orgname\", type = \"institution\")))\n",
    "                affiliations.append(aff)\n",
    "        assert(len(authors_list)==len(affiliations))\n",
    "        assert(len(authors_list)==len(emails))\n",
    "        for i in range(len(authors_list)):\n",
    "            firstname, middlename, surname = authors_list[i]\n",
    "            author = Author_G(firstname, middlename, surname, affiliations[i], emails[i])\n",
    "            result.append(author)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple affiliations for a person cannot be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the Bubble Go Beyond? An Exploration of the Urban Filter Bubble\n",
      "[Author_G(firstname='Annelien', middlename='', surname='Smets', affiliation='imec-SMIT Vrije Universiteit Brussel', email=''), Author_G(firstname='Eladio', middlename='', surname='Montero', affiliation='imec-SMIT Vrije Universiteit Brussel', email=''), Author_G(firstname='Pieter', middlename='', surname='Ballon', affiliation='imec-SMIT Vrije Universiteit Brussel', email='')]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hans.vrapi\\AppData\\Local\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for k in papers.keys():\n",
    "    for p in papers[k]:\n",
    "        filename = f'http://ceurspt.wikidata.dbis.rwth-aachen.de/Vol-{k}/paper{p}.grobid'\n",
    "        # extract metadata for each paper using CERMINE and GROBID (provided through API), including title, authors, affiliations, publication year\n",
    "        grobid =  GrobitFile(filename)\n",
    "        print(grobid.title)\n",
    "        print(grobid.authors)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with dblp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hans.vrapi\\AppData\\Local\\anaconda3\\lib\\site-packages\\dblp-0.1.0-py3.10.egg\\dblp\\__init__.py:19: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 19 of the file c:\\Users\\hans.vrapi\\AppData\\Local\\anaconda3\\lib\\site-packages\\dblp-0.1.0-py3.10.egg\\dblp\\__init__.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Link</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Title</th>\n",
       "      <th>Where</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>https://ceur-ws.org/Vol-2462/paper2.pdf</td>\n",
       "      <td>[Yashar Deldjoo, Tommaso Di Noia, Felice Anton...</td>\n",
       "      <td>Assessing the Impact of a User-Item Collaborat...</td>\n",
       "      <td>ImpactRS@RecSys</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>informal</td>\n",
       "      <td>http://arxiv.org/abs/1908.07968</td>\n",
       "      <td>[Yashar Deldjoo, Tommaso Di Noia, Felice Anton...</td>\n",
       "      <td>Assessing the Impact of a User-Item Collaborat...</td>\n",
       "      <td>CoRR</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type                                     Link  \\\n",
       "0  inproceedings  https://ceur-ws.org/Vol-2462/paper2.pdf   \n",
       "1       informal          http://arxiv.org/abs/1908.07968   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  [Yashar Deldjoo, Tommaso Di Noia, Felice Anton...   \n",
       "1  [Yashar Deldjoo, Tommaso Di Noia, Felice Anton...   \n",
       "\n",
       "                                               Title            Where  Year  \n",
       "0  Assessing the Impact of a User-Item Collaborat...  ImpactRS@RecSys  2019  \n",
       "1  Assessing the Impact of a User-Item Collaborat...             CoRR  2019  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = dblp.search([grobid.title])\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERMINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assessing the Impact of a User-Item Collaborative Atack on Class of Users∗'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_file = BeautifulSoup(req.get('http://ceurspt.wikidata.dbis.rwth-aachen.de/Vol-2462/paper2.cermine').text, 'lxml')\n",
    "article_title = xml_file.find('article-title').text\n",
    "article_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Author_C:\n",
    "    fullname: str\n",
    "    affiliation: str\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CermineFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.cermine = BeautifulSoup(req.get(filename).text, 'lxml')\n",
    "        self._title = ''\n",
    "\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self._title:\n",
    "            self._title = elem_to_text(self.cermine.find('article-title'))\n",
    "        return self._title\n",
    "\n",
    "\n",
    "    @property\n",
    "    def authors(self):\n",
    "        authors_in_header = self.cermine.find('article-meta').find('contrib-group').find_all('contrib')\n",
    "        result = []\n",
    "        for author in authors_in_header:\n",
    "            name = elem_to_text(author.find('string-name'))\n",
    "            email = []\n",
    "            for e in author.findAll('email'):\n",
    "                email.append(elem_to_text(e))\n",
    "            #email = elem_to_text(author.email)\n",
    "            xref_aff_id = 'aff' + elem_to_text(author.xref)\n",
    "        \n",
    "            aff_tag = self.cermine.find('article-meta').find('contrib-group').find('aff', {'id': xref_aff_id})\n",
    "            affiliation = []\n",
    "            if aff_tag:\n",
    "                for a in aff_tag.findAll('institution'):\n",
    "                    affiliation += [elem_to_text(a)]\n",
    "            else:\n",
    "                print(f\"Author: {name}, Institution not found\")\n",
    "\n",
    "            author = Author_C(name, affiliation, email)\n",
    "            result.append(author)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emails dont work properly for cermine /paper 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing the Impact of a User-Item Collaborative Atack on Class of Users∗\n",
      "[Author_C(fullname='Yashar Deldjoo', affiliation=['Polytechnic University of Bari'], email=['R@k', 'yashar.deldjoo@poliba.it']), Author_C(fullname='Tommaso Di Noia', affiliation=['Polytechnic University of Bari'], email=['tommaso.dinoia@poliba.it']), Author_C(fullname='Felice Antonio Merra†', affiliation=['Polytechnic University of Bari'], email=['felice.merra@poliba.it'])]\n"
     ]
    }
   ],
   "source": [
    "filename = 'http://ceurspt.wikidata.dbis.rwth-aachen.de/Vol-2462/paper2.cermine'\n",
    "grobid =  CermineFile(filename)\n",
    "print(grobid.title)\n",
    "print(grobid.authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
